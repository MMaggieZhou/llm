{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgZpTckYfyimknceHv4yAZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMaggieZhou/llm/blob/main/Multi_Head_Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rezkx1-2xequ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Implementation choises:\n",
        "Projections of multi-heads are concatinated into one dimension;\n",
        "the calculated Q, K, V are then reshaped to have head number in new dimension\n",
        "before calculating attention score\n",
        "Number of flops ~\n",
        "O(batch_size * sequence_l * dmodel^2) +\n",
        "O(batch_size * dmodel * sequence_l^2)\n",
        "'''\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, dmodel, dk, h):\n",
        "      super().__init__() #?\n",
        "      assert dmodel % h == 0, \"Embedding dimension must be divisible by the number of heads\"\n",
        "      self.h = h\n",
        "      self.dmodel = dmodel\n",
        "      self.dk = dk\n",
        "      self.dv = self.dmodel // self.h\n",
        "\n",
        "      self.Q = nn.Linear(dmodel, dk * h)\n",
        "      self.K = nn.Linear(dmodel, dk * h)\n",
        "      self.V = nn.Linear(dmodel, dmodel)\n",
        "\n",
        "    # break q, k, v into heads\n",
        "    def _reshape(self, t):\n",
        "        new_shape = t.size()[:-1] + (self.h, t.size()[-1] // self.h)\n",
        "        t = t.view(new_shape) # (batch_size, sequence_l, h, dk or dv)\n",
        "        return t.permute(0,2,1,3) # (batch_size, h, sequence_l, dk or dv)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        attention_mask=None, # all encoders share of a batch share the same mask and same applies to decoders\n",
        "      ):\n",
        "      # O(batch_size * sequence_l * dk * h * dmodel)\n",
        "      Q = self._reshape(self.Q(x)) # (batch_size, h, sequence_l, dk)\n",
        "      # O(batch_size * sequence_l * dk * h * dmodel)\n",
        "      K = self._reshape(self.K(x)) # (batch_size, h, sequence_l, dk)\n",
        "      # O(batch_size * sequence_l * dmodel^2)\n",
        "      V = self._reshape(self.V(x)) # (batch_size, h, sequence_l, dv)\n",
        "\n",
        "      # softmax(QK/dv-2)V, O(batch_size * h * dk * sequence_l ^ 2)\n",
        "      scores = torch.matmul(Q, K.permute(0,1,3,2)) / math.sqrt(self.dk) #(batch_size, h, sequence_l, sequence_l)\n",
        "      if attention_mask != None:  # (batch_size, 1, sequence_l, sequence_l)\n",
        "          scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
        "      probs = F.softmax(scores, dim=-1)\n",
        "      # O(batch_size * dmodel * sequence_l^2)\n",
        "      output = torch.matmul(probs, V) # (batch_size, h, sequence_l, dv)\n",
        "\n",
        "      # concat\n",
        "      output = output.permute(0,2,1,3).contiguous() #? error view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
        "      new_shape = output.size()[:-2] + (self.dmodel,)\n",
        "      return output.view(new_shape), scores # output: (batch_size, sequene_l, dmodel), return scores for debugging"
      ],
      "metadata": {
        "id": "fYI9ma4_xrVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Position-wise Feed-Forward Networks:\n",
        "\n",
        "O(batch_size * sequence_l * dmodel * dff)\n",
        "\n",
        "Final output layer:\n",
        "O(batch_size * sequence_l * dmodel * #tokens)"
      ],
      "metadata": {
        "id": "PH4nHJkDKNG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to prevent attending to invalid tokens, aka padding tokens or future token;\n",
        "def generate_attention_mask(embedding_mask, is_decoder):\n",
        "    batch_size, seq_length = embedding_mask.size()\n",
        "    mask = embedding_mask.unsqueeze(1).unsqueeze(2).expand(batch_size, 1, seq_length, seq_length)\n",
        "\n",
        "    if is_decoder:\n",
        "        tril = torch.tril(torch.ones(seq_length, seq_length))  # 0s where j > i, 1s elsewhere\n",
        "        mask = mask.masked_fill(tril==0, 0)\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "eSzGQ-RNlkSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "seq_length = 5\n",
        "dmodel = 64\n",
        "h = 8\n",
        "dk = 10\n",
        "is_decoder = True\n",
        "\n",
        "multihead_attn = MultiHeadSelfAttention(dmodel, dk, h)\n",
        "x = torch.rand(batch_size, seq_length, dmodel)\n",
        "# the first sentence has 5 tokens and second sentence has 3 tokens\n",
        "embedding_mask = torch.tensor([[1,1,1,1,1],[1,1,1,0,0]])\n",
        "attention_mask = generate_attention_mask(embedding_mask, is_decoder)\n",
        "output, scores = multihead_attn(x, attention_mask)\n",
        "print(output.size())\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJbcrAxxwa7V",
        "outputId": "389ad049-cc51-4025-8183-7afa951f35d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 64])\n",
            "tensor([[[[ 0.1050,    -inf,    -inf,    -inf,    -inf],\n",
            "          [ 0.1195,  0.0612,    -inf,    -inf,    -inf],\n",
            "          [ 0.0789,  0.0508,  0.0496,    -inf,    -inf],\n",
            "          [-0.0016,  0.0775,  0.0460,  0.0938,    -inf],\n",
            "          [ 0.0679,  0.0545,  0.0227,  0.0369,  0.0350]],\n",
            "\n",
            "         [[ 0.1365,    -inf,    -inf,    -inf,    -inf],\n",
            "          [ 0.0953, -0.0316,    -inf,    -inf,    -inf],\n",
            "          [ 0.1592,  0.0084,  0.0808,    -inf,    -inf],\n",
            "          [ 0.0997, -0.0295,  0.0350,  0.0579,    -inf],\n",
            "          [ 0.1123,  0.0090,  0.0596,  0.0739,  0.0521]],\n",
            "\n",
            "         [[ 0.0861,    -inf,    -inf,    -inf,    -inf],\n",
            "          [ 0.1755,  0.0604,    -inf,    -inf,    -inf],\n",
            "          [ 0.1689,  0.1097,  0.1334,    -inf,    -inf],\n",
            "          [ 0.1076,  0.0564,  0.0125,  0.0753,    -inf],\n",
            "          [ 0.0549,  0.0259, -0.0416, -0.0207, -0.1313]],\n",
            "\n",
            "         [[ 0.0301,    -inf,    -inf,    -inf,    -inf],\n",
            "          [-0.0230, -0.2002,    -inf,    -inf,    -inf],\n",
            "          [-0.0273, -0.1311, -0.1433,    -inf,    -inf],\n",
            "          [ 0.0447, -0.1216, -0.0945, -0.0421,    -inf],\n",
            "          [-0.0093, -0.1237, -0.1408, -0.1361,  0.0060]],\n",
            "\n",
            "         [[ 0.1947,    -inf,    -inf,    -inf,    -inf],\n",
            "          [ 0.2057,  0.2083,    -inf,    -inf,    -inf],\n",
            "          [ 0.1010,  0.1541,  0.0462,    -inf,    -inf],\n",
            "          [ 0.1896,  0.1755,  0.0538,  0.1949,    -inf],\n",
            "          [ 0.2022,  0.2216,  0.0742,  0.1888,  0.2029]],\n",
            "\n",
            "         [[-0.1004,    -inf,    -inf,    -inf,    -inf],\n",
            "          [-0.0603, -0.0208,    -inf,    -inf,    -inf],\n",
            "          [-0.0550, -0.0249, -0.1243,    -inf,    -inf],\n",
            "          [-0.0437, -0.0486, -0.1778, -0.0036,    -inf],\n",
            "          [-0.0681, -0.0736, -0.2185, -0.1130, -0.0733]],\n",
            "\n",
            "         [[-0.1906,    -inf,    -inf,    -inf,    -inf],\n",
            "          [-0.1924, -0.2263,    -inf,    -inf,    -inf],\n",
            "          [-0.2203, -0.2624, -0.1782,    -inf,    -inf],\n",
            "          [-0.1520, -0.2242, -0.0923, -0.1782,    -inf],\n",
            "          [-0.2116, -0.2628, -0.2466, -0.1579, -0.2818]],\n",
            "\n",
            "         [[ 0.0543,    -inf,    -inf,    -inf,    -inf],\n",
            "          [ 0.0215,  0.1058,    -inf,    -inf,    -inf],\n",
            "          [ 0.0569,  0.0774, -0.0008,    -inf,    -inf],\n",
            "          [ 0.0154,  0.0709,  0.0237,  0.0390,    -inf],\n",
            "          [-0.0466,  0.0463,  0.0629,  0.0603, -0.0435]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1289,    -inf,    -inf,    -inf,    -inf],\n",
            "          [-0.0179, -0.1111,    -inf,    -inf,    -inf],\n",
            "          [ 0.0185, -0.0659, -0.0024,    -inf,    -inf],\n",
            "          [ 0.0662, -0.0191,  0.0469,    -inf,    -inf],\n",
            "          [ 0.0916,  0.0228,  0.1347,    -inf,    -inf]],\n",
            "\n",
            "         [[ 0.0466,    -inf,    -inf,    -inf,    -inf],\n",
            "          [ 0.0828,  0.1717,    -inf,    -inf,    -inf],\n",
            "          [ 0.1428,  0.1453,  0.0751,    -inf,    -inf],\n",
            "          [ 0.0796,  0.1380,  0.0309,    -inf,    -inf],\n",
            "          [ 0.0318,  0.1725, -0.0288,    -inf,    -inf]],\n",
            "\n",
            "         [[ 0.1128,    -inf,    -inf,    -inf,    -inf],\n",
            "          [-0.0399,  0.0020,    -inf,    -inf,    -inf],\n",
            "          [ 0.0879, -0.0057,  0.0387,    -inf,    -inf],\n",
            "          [ 0.0254,  0.0063, -0.0095,    -inf,    -inf],\n",
            "          [ 0.0803, -0.0201,  0.0356,    -inf,    -inf]],\n",
            "\n",
            "         [[ 0.0432,    -inf,    -inf,    -inf,    -inf],\n",
            "          [ 0.0128, -0.0807,    -inf,    -inf,    -inf],\n",
            "          [ 0.0067, -0.0815, -0.0656,    -inf,    -inf],\n",
            "          [-0.0010, -0.1205, -0.1199,    -inf,    -inf],\n",
            "          [-0.0129, -0.0958, -0.1531,    -inf,    -inf]],\n",
            "\n",
            "         [[ 0.1870,    -inf,    -inf,    -inf,    -inf],\n",
            "          [ 0.0436,  0.1252,    -inf,    -inf,    -inf],\n",
            "          [-0.0050,  0.0445, -0.0606,    -inf,    -inf],\n",
            "          [ 0.2551,  0.2344,  0.0997,    -inf,    -inf],\n",
            "          [ 0.1573,  0.2037,  0.0933,    -inf,    -inf]],\n",
            "\n",
            "         [[-0.1483,    -inf,    -inf,    -inf,    -inf],\n",
            "          [-0.1037, -0.1495,    -inf,    -inf,    -inf],\n",
            "          [-0.1739, -0.2110, -0.0383,    -inf,    -inf],\n",
            "          [-0.1616, -0.2099,  0.0545,    -inf,    -inf],\n",
            "          [-0.1093, -0.2276,  0.0377,    -inf,    -inf]],\n",
            "\n",
            "         [[-0.2649,    -inf,    -inf,    -inf,    -inf],\n",
            "          [-0.1949, -0.2683,    -inf,    -inf,    -inf],\n",
            "          [-0.2036, -0.2340, -0.1768,    -inf,    -inf],\n",
            "          [-0.2174, -0.2315, -0.1607,    -inf,    -inf],\n",
            "          [-0.2152, -0.2278, -0.1532,    -inf,    -inf]],\n",
            "\n",
            "         [[ 0.0427,    -inf,    -inf,    -inf,    -inf],\n",
            "          [ 0.0003,  0.0870,    -inf,    -inf,    -inf],\n",
            "          [ 0.0713,  0.0796,  0.0979,    -inf,    -inf],\n",
            "          [ 0.0789,  0.0736,  0.0880,    -inf,    -inf],\n",
            "          [ 0.0070, -0.0057,  0.0846,    -inf,    -inf]]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    }
  ]
}