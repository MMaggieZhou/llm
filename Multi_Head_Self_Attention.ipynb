{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCBmubMeKDjQsljvVbVXsM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMaggieZhou/llm/blob/main/Multi_Head_Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "Rezkx1-2xequ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "TODO: make parameters consistent with the paper\n",
        "'''\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, k_dim, num_heads):\n",
        "      super().__init__() #?\n",
        "      assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads\"\n",
        "      self.num_heads = num_heads\n",
        "      self.embed_dim = embed_dim\n",
        "      self.k_dim = k_dim\n",
        "      self.v_dim = self.embed_dim // self.num_heads\n",
        "\n",
        "      self.query = nn.Linear(embed_dim, k_dim * num_heads)\n",
        "      self.key = nn.Linear(embed_dim, k_dim * num_heads)\n",
        "      self.value = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    # break q, k, v into heads\n",
        "    def _reshape(self, t):\n",
        "        new_shape = t.size()[:-1] + (self.num_heads, t.size()[-1] // self.num_heads) # TODO: size() or shape?\n",
        "        t = t.view(new_shape) # (batch_size, sequence_l, num_heads, k_dim or v_dim)\n",
        "        return t.permute(0,2,1,3) # (batch_size, num_heads, sequence_l, k_dim or v_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        attention_mask=None, # all encoders share of a batch share the same mask and same applies to decoders\n",
        "      ):\n",
        "\n",
        "      Q = self._reshape(self.query(x)) # (batch_size, num_heads, sequence_l, k_dim)\n",
        "      K = self._reshape(self.key(x)) # (batch_size, num_heads, sequence_l, k_dim)\n",
        "      V = self._reshape(self.value(x)) # (batch_size, num_heads, sequence_l, v_dim)\n",
        "\n",
        "      # softmax(QK/v_dim-2)V\n",
        "      scores = torch.matmul(Q, K.permute(0,1,3,2)) / math.sqrt(self.k_dim) #(batch_size, num_heads, sequence_l, sequence_l)\n",
        "      if attention_mask != None:  # (batch_size, 1, sequence_l, sequence_l)\n",
        "          print(scores.size())\n",
        "          print(attention_mask.size())\n",
        "          scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
        "      probs = F.softmax(scores, dim=-1)\n",
        "      output = torch.matmul(probs, V) # (batch_size, num_heads, sequence_l, v_dim)\n",
        "\n",
        "      # concat\n",
        "      output = output.permute(0,2,1,3).contiguous() #? error view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
        "      new_shape = output.size()[:-2] + (self.embed_dim,)\n",
        "      return output.view(new_shape), scores # (batch_size, sequene_l, embed_dim)"
      ],
      "metadata": {
        "id": "fYI9ma4_xrVA"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to prevent attending to invalid tokens, aka padding tokens or future token;\n",
        "def generate_attention_mask(embedding_mask, is_decoder):\n",
        "    batch_size, seq_length = embedding_mask.size()\n",
        "    mask = embedding_mask.unsqueeze(1).unsqueeze(2).expand(batch_size, 1, seq_length, seq_length)\n",
        "\n",
        "    if is_decoder:\n",
        "        tril = torch.tril(torch.ones(seq_length, seq_length))  # 0s where j > i, 1s elsewhere\n",
        "        mask = mask.masked_fill(tril==0, 0)\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "eSzGQ-RNlkSR"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "seq_length = 5\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "k_dim = 10\n",
        "is_decoder = True\n",
        "\n",
        "multihead_attn = MultiHeadSelfAttention(embed_dim, k_dim, num_heads)\n",
        "x = torch.rand(batch_size, seq_length, embed_dim)\n",
        "# the first sentence has 5 tokens and second sentence has 3 tokens\n",
        "embedding_mask = torch.tensor([[1,1,1,1,1],[1,1,1,0,0]])\n",
        "attention_mask = generate_attention_mask(embedding_mask, is_decoder)\n",
        "output, scores = multihead_attn(x, attention_mask)\n",
        "print(output.size())\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJbcrAxxwa7V",
        "outputId": "9ab24bd8-a9a8-4d29-c1e2-868e4c13bf12"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 8, 5, 5])\n",
            "torch.Size([2, 1, 5, 5])\n",
            "torch.Size([2, 5, 64])\n",
            "tensor([[[[ 1.4343e-02,        -inf,        -inf,        -inf,        -inf],\n",
            "          [-1.2957e-02,  1.2561e-02,        -inf,        -inf,        -inf],\n",
            "          [-3.0356e-03,  7.3240e-02, -1.8429e-02,        -inf,        -inf],\n",
            "          [ 6.8252e-02,  1.3929e-01,  1.2465e-01,  2.2523e-01,        -inf],\n",
            "          [ 7.6309e-02,  1.2280e-01,  9.6493e-02,  1.7525e-01,  7.9725e-02]],\n",
            "\n",
            "         [[-2.4399e-03,        -inf,        -inf,        -inf,        -inf],\n",
            "          [-1.2792e-03, -1.2840e-03,        -inf,        -inf,        -inf],\n",
            "          [-4.1264e-02, -3.5341e-02,  7.2489e-02,        -inf,        -inf],\n",
            "          [-1.2637e-01, -8.6460e-02, -3.6553e-02,  6.5681e-02,        -inf],\n",
            "          [-3.1637e-02, -1.7299e-02,  4.9774e-02,  8.0568e-02, -3.4710e-02]],\n",
            "\n",
            "         [[ 5.3687e-02,        -inf,        -inf,        -inf,        -inf],\n",
            "          [ 9.9212e-02, -1.0597e-02,        -inf,        -inf,        -inf],\n",
            "          [ 6.1403e-02,  1.1729e-02,  1.2819e-01,        -inf,        -inf],\n",
            "          [ 1.6273e-01,  6.0612e-02,  1.3763e-02,  1.1166e-01,        -inf],\n",
            "          [ 9.5172e-02,  1.6111e-02,  3.3725e-02,  8.7872e-02,  1.2727e-01]],\n",
            "\n",
            "         [[ 9.5772e-02,        -inf,        -inf,        -inf,        -inf],\n",
            "          [ 2.9364e-02,  3.2143e-02,        -inf,        -inf,        -inf],\n",
            "          [ 4.6947e-02,  1.1918e-01,  5.6907e-02,        -inf,        -inf],\n",
            "          [-3.9805e-03,  2.2356e-02, -2.5272e-02,  3.9421e-02,        -inf],\n",
            "          [-2.6099e-04,  5.8925e-02,  3.2014e-02,  4.8339e-02,  1.5299e-02]],\n",
            "\n",
            "         [[-3.9968e-02,        -inf,        -inf,        -inf,        -inf],\n",
            "          [ 3.8053e-02, -1.3248e-01,        -inf,        -inf,        -inf],\n",
            "          [-5.5060e-03, -1.5912e-01,  4.2335e-02,        -inf,        -inf],\n",
            "          [ 8.3257e-02, -1.3745e-01,  1.3998e-01,  7.4637e-02,        -inf],\n",
            "          [ 1.8867e-02, -1.6634e-01,  8.9032e-02,  1.3282e-02, -6.2264e-02]],\n",
            "\n",
            "         [[ 1.8238e-01,        -inf,        -inf,        -inf,        -inf],\n",
            "          [ 9.4927e-02,  1.1294e-01,        -inf,        -inf,        -inf],\n",
            "          [ 5.6895e-03,  8.6299e-02,  1.2073e-01,        -inf,        -inf],\n",
            "          [ 8.9142e-02,  1.5943e-01,  1.6807e-01,  1.7923e-01,        -inf],\n",
            "          [ 1.8385e-01,  2.8425e-01,  2.3813e-01,  2.3292e-01,  2.4146e-01]],\n",
            "\n",
            "         [[-6.2553e-02,        -inf,        -inf,        -inf,        -inf],\n",
            "          [-5.0284e-02, -6.6672e-02,        -inf,        -inf,        -inf],\n",
            "          [-3.4125e-03,  1.3148e-02, -1.4406e-02,        -inf,        -inf],\n",
            "          [-8.8852e-02, -7.6236e-02, -8.0856e-02, -4.9288e-02,        -inf],\n",
            "          [-4.4658e-03, -4.1841e-02, -7.0834e-03, -5.5938e-03, -5.4165e-02]],\n",
            "\n",
            "         [[-9.8853e-02,        -inf,        -inf,        -inf,        -inf],\n",
            "          [-8.7792e-02, -7.3634e-02,        -inf,        -inf,        -inf],\n",
            "          [-2.2395e-01, -2.0342e-01, -6.1231e-02,        -inf,        -inf],\n",
            "          [-1.8330e-01, -1.1272e-01,  9.6907e-04, -8.7803e-02,        -inf],\n",
            "          [-1.1636e-01, -9.4427e-02,  1.1562e-02, -7.1502e-02, -7.6493e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3225e-01,        -inf,        -inf,        -inf,        -inf],\n",
            "          [ 1.7183e-01,  2.2612e-01,        -inf,        -inf,        -inf],\n",
            "          [ 2.5513e-01,  2.6510e-01,  1.3329e-01,        -inf,        -inf],\n",
            "          [ 1.6991e-01,  2.0342e-01,  8.0847e-02,        -inf,        -inf],\n",
            "          [ 1.6338e-01,  1.5337e-01,  6.6755e-02,        -inf,        -inf]],\n",
            "\n",
            "         [[ 8.3508e-02,        -inf,        -inf,        -inf,        -inf],\n",
            "          [ 1.2941e-01,  1.8840e-01,        -inf,        -inf,        -inf],\n",
            "          [ 6.6570e-02,  7.8475e-02, -4.5244e-02,        -inf,        -inf],\n",
            "          [ 3.9899e-02,  1.1164e-01, -5.5252e-02,        -inf,        -inf],\n",
            "          [ 4.8533e-02,  2.5507e-02, -4.9307e-02,        -inf,        -inf]],\n",
            "\n",
            "         [[ 1.0493e-01,        -inf,        -inf,        -inf,        -inf],\n",
            "          [ 8.1581e-02,  1.2802e-01,        -inf,        -inf,        -inf],\n",
            "          [ 9.0148e-02,  1.5112e-01,  1.2783e-02,        -inf,        -inf],\n",
            "          [ 1.3104e-01,  1.5144e-01,  2.6924e-02,        -inf,        -inf],\n",
            "          [-4.7353e-02,  1.9028e-02, -9.1012e-02,        -inf,        -inf]],\n",
            "\n",
            "         [[-8.2029e-03,        -inf,        -inf,        -inf,        -inf],\n",
            "          [ 9.0994e-02,  9.5652e-02,        -inf,        -inf,        -inf],\n",
            "          [ 4.5409e-02,  2.5704e-02,  2.3141e-02,        -inf,        -inf],\n",
            "          [-1.4224e-01, -9.1027e-02, -1.0412e-01,        -inf,        -inf],\n",
            "          [-4.2242e-03,  2.9481e-03,  1.2538e-02,        -inf,        -inf]],\n",
            "\n",
            "         [[-5.1995e-02,        -inf,        -inf,        -inf,        -inf],\n",
            "          [-1.3570e-01, -3.6313e-02,        -inf,        -inf,        -inf],\n",
            "          [-2.1038e-01, -1.0354e-01, -8.0159e-02,        -inf,        -inf],\n",
            "          [-8.3371e-02,  1.0306e-01,  4.4170e-02,        -inf,        -inf],\n",
            "          [-1.5214e-01, -2.4586e-02, -4.0809e-02,        -inf,        -inf]],\n",
            "\n",
            "         [[ 2.5732e-01,        -inf,        -inf,        -inf,        -inf],\n",
            "          [ 2.2078e-01,  3.0223e-01,        -inf,        -inf,        -inf],\n",
            "          [ 1.3951e-01,  2.5017e-01,  1.9958e-01,        -inf,        -inf],\n",
            "          [ 1.6532e-01,  3.1604e-01,  2.1197e-01,        -inf,        -inf],\n",
            "          [ 1.5410e-01,  2.4571e-01,  1.5939e-01,        -inf,        -inf]],\n",
            "\n",
            "         [[ 9.4543e-03,        -inf,        -inf,        -inf,        -inf],\n",
            "          [-5.6221e-02, -3.8938e-02,        -inf,        -inf,        -inf],\n",
            "          [-1.3820e-01, -1.0146e-01, -1.2550e-01,        -inf,        -inf],\n",
            "          [-1.4678e-01, -1.7023e-01, -1.5332e-01,        -inf,        -inf],\n",
            "          [-8.2597e-02, -1.0982e-01, -9.7888e-02,        -inf,        -inf]],\n",
            "\n",
            "         [[-2.1801e-01,        -inf,        -inf,        -inf,        -inf],\n",
            "          [-1.6638e-01, -6.5664e-02,        -inf,        -inf,        -inf],\n",
            "          [-1.0798e-01,  3.2046e-02, -1.2245e-02,        -inf,        -inf],\n",
            "          [-8.6804e-02,  1.2565e-01,  4.9875e-02,        -inf,        -inf],\n",
            "          [-1.3435e-01,  4.6634e-02,  5.5393e-03,        -inf,        -inf]]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    }
  ]
}